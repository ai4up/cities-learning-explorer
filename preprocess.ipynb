{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.resetwarnings()\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(funcName)s - %(message)s', datefmt='%I:%M:%S')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Climate solution counts per domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_solutions = pd.read_parquet('../cities-learning-dec/data/climate_solutions_typology/oa_sentence_solutions_2.parquet')\n",
    "abstract_city_mapping = pd.read_csv('../cities-learning-dec/data/geoparser/clean_places_augmented_dedup.csv', index_col=0)\n",
    "\n",
    "abstract_solutions = pd.merge(abstract_solutions, abstract_city_mapping[['id', 'city_id']], on='id')\n",
    "\n",
    "domain_to_solution_ids = {\n",
    "    \"Mobility\": [1, 2, 3, 4, 5, 6, 7],\n",
    "    \"Buildings\": [8, 9, 10, 11, 12, 13, 14, 15, 16],\n",
    "    \"Energy\": [17, 18, 19, 20, 21, 22, 23],\n",
    "    \"Thermal comfort and Heat stress management\": [24, 25, 26, 27],\n",
    "    \"Food provisioning systems\": [28, 29, 30, 31, 32, 33, 34],\n",
    "    \"Water\": [35, 36, 37, 38, 39, 40],\n",
    "    \"Waste management\": [41, 42, 43, 44],\n",
    "    \"Disaster and risk management\": [45, 46, 47, 48, 49, 50],\n",
    "    \"Carbon dioxide removal\": [51, 52, 53, 54],\n",
    "}\n",
    "\n",
    "for domain, solution_ids in domain_to_solution_ids.items():\n",
    "    abstract_solutions[domain] = abstract_solutions[[f'solution_{i}_match' for i in solution_ids]].max(axis=1)\n",
    "\n",
    "domains = list(domain_to_solution_ids.keys())\n",
    "abstract_solution_counts = abstract_solutions.groupby(['city_id','id'])[domains].max()\n",
    "city_solution_counts = abstract_solution_counts.groupby('city_id')[domains].sum()\n",
    "\n",
    "city_solution = pd.DataFrame({\n",
    "    'GHS_urban_area_id': city_solution_counts.index,\n",
    "    'n_solutions': city_solution_counts.sum(axis=1),\n",
    "    'solution_domain_counts': city_solution_counts.to_dict(orient=\"records\"),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Nearest neighbors in embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for numpy version compatibility issues\n",
    "\n",
    "import sys\n",
    "import types\n",
    "import numpy as np\n",
    "\n",
    "# Create fake parent module\n",
    "np__core = types.ModuleType(\"numpy._core\")\n",
    "\n",
    "# Map numpy._core.multiarray -> numpy.core.multiarray\n",
    "np__core.multiarray = np.core.multiarray\n",
    "\n",
    "# Inject modules into sys.modules\n",
    "sys.modules[\"numpy._core\"] = np__core\n",
    "sys.modules[\"numpy._core.multiarray\"] = np__core.multiarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_all_runs = []\n",
    "for i in range(30):\n",
    "    embeddings = joblib.load(f'../cities-learning-dec/clustering_models/latent_representation/latent_run_{i}.pkl')\n",
    "\n",
    "    emb_cols = [\"latent_0\", \"latent_1\", \"latent_2\", \"latent_3\"]\n",
    "    X = embeddings[emb_cols].values\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=51, metric=\"euclidean\")\n",
    "    nn.fit(X)\n",
    "\n",
    "    distances, indices = nn.kneighbors(X)\n",
    "\n",
    "    # Remove self-index (first column)\n",
    "    neighbor_indices = indices[:, 1:]\n",
    "    neighbor_distances = distances[:, 1:]\n",
    "\n",
    "    city_ids = embeddings[\"GHS_urban_area_id\"].astype(int).values\n",
    "\n",
    "\n",
    "    N, K = neighbor_indices.shape\n",
    "\n",
    "    # Repeat each city ID K times â†’ shape (N*K,)\n",
    "    city_a = np.repeat(city_ids, K)\n",
    "\n",
    "    # Flatten neighbors and distances (already in the correct order)\n",
    "    city_b = city_ids[neighbor_indices].ravel()\n",
    "    dist = neighbor_distances.ravel()\n",
    "\n",
    "    nn_long = pd.DataFrame({\n",
    "        \"city_a\": city_a,\n",
    "        \"city_b\": city_b,\n",
    "        \"distance\": dist\n",
    "    })\n",
    "    nn_all_runs.append(nn_long)\n",
    "\n",
    "nn_long = pd.concat(nn_all_runs)\n",
    "\n",
    "nn = (\n",
    "    nn_long.groupby(['city_a', 'city_b'], as_index=False)\n",
    "          .distance.mean()\n",
    ")\n",
    "\n",
    "nn_top20 = (\n",
    "    nn.sort_values(['city_a', 'distance'])\n",
    "     .groupby('city_a')\n",
    "     .head(20)\n",
    "     .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "nn_top20['distance'] = nn_top20['distance'].round(4)\n",
    "nn_top20['distance'] = nn_top20['distance'].round(4)\n",
    "\n",
    "nn_wide = (\n",
    "    nn_top20\n",
    "    .groupby(\"city_a\")\n",
    "    .agg({\n",
    "        \"city_b\": list,\n",
    "        \"distance\": list\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "nn_wide = nn_wide.rename(columns={\n",
    "    \"city_a\": \"GHS_urban_area_id\",\n",
    "    \"city_b\": \"neighbors\",\n",
    "    \"distance\": \"neighbor_distances\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging all data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_meta = pd.read_csv('../cities-learning-dec/data/clustering_results/cities_by_regional_type_clean.csv', index_col=0).rename(columns={'city_id': 'GHS_urban_area_id'})[['GHS_urban_area_id', 'city_name', 'country', 'Region', 'n_studies']]\n",
    "city_characteristics = pd.read_parquet('../cities-learning-dec/data/clustering_data_clean/GHS_UCDB_2024_preproc_2025_04_09_uci_and_nan_imputation_add_vars_included.parquet')\n",
    "embeddings = joblib.load('../cities-learning-dec/clustering_models/latent_representation/latent_run_0.pkl')\n",
    "city_socioeconomics = pd.read_csv('../cities-learning-dec/data/GHS_UCDB_GLOBE_R2024A_V1_0/socioeconomic.csv')[['ID_UC_G0', 'SC_SEC_GDF_2020', 'SC_SEC_HDI_2020']].rename(columns={'ID_UC_G0': 'GHS_urban_area_id', 'SC_SEC_GDF_2020': 'GHS_female_gender_index', 'SC_SEC_HDI_2020': 'GHS_HDI'})\n",
    "\n",
    "city_assignment_proba = pd.read_csv('../cities-learning-dec/data/clustering_results/dec_clusters_k4.csv')\n",
    "city_assignment_proba['probability'] = city_assignment_proba[['mean_prob_cluster_0', 'mean_prob_cluster_1', 'mean_prob_cluster_2', 'mean_prob_cluster_3']].max(axis=1)\n",
    "mixed_thresholds = city_assignment_proba.groupby('consensus_label_majority')['probability'].median() * 0.65\n",
    "city_assignment_proba['mixed'] = city_assignment_proba['probability'] < city_assignment_proba['consensus_label_majority'].map(mixed_thresholds)\n",
    "city_assignment_proba['type'] = city_assignment_proba['consensus_label_majority'].map({0: \"Type 2\", 1: \"Type 3\", 2: \"Type 1\", 3: \"Type 4\"})\n",
    "city_assignment_proba.loc[city_assignment_proba['mixed'], 'type'] = \"Mixed\"\n",
    "\n",
    "emissions = pd.read_csv('../cities-learning-dec/data/emissions/balance_sheet.csv')\n",
    "emissions = emissions[emissions['Year'] == 2022][['ID_UC_G0', 'ODIAC']].rename(columns={'ID_UC_G0': 'GHS_urban_area_id', 'ODIAC': 'total_emissions'})\n",
    "\n",
    "geometries = gpd.read_parquet('../cities-learning-dec/data/clustering_data_clean/GHS_UCDB_2024_preproc_2025_04_03.parquet', columns=['GHS_urban_area_id', 'geometry'])\n",
    "geometries['lat'] = geometries.centroid.to_crs(epsg=4326).y\n",
    "geometries['lon'] = geometries.centroid.to_crs(epsg=4326).x\n",
    "geometries = geometries[['GHS_urban_area_id', 'lat', 'lon']]\n",
    "\n",
    "cities = pd.merge(city_characteristics, city_meta, on='GHS_urban_area_id')\n",
    "cities = pd.merge(city_assignment_proba, cities, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, embeddings, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, nn_wide, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, city_socioeconomics, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, geometries, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, city_solution, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, emissions, on='GHS_urban_area_id', how='left')\n",
    "\n",
    "cities['emissions'] = cities['total_emissions'] / cities['GHS_population']\n",
    "cities['city_name'].fillna('Unknown', inplace=True)\n",
    "cities['embedding'] = cities[['latent_0', 'latent_1', 'latent_2', 'latent_3']].to_numpy().tolist()\n",
    "# cities['type_probabilities'] = cities[['mean_prob_cluster_1', 'mean_prob_cluster_2', 'mean_prob_cluster_3', 'mean_prob_cluster_0']].to_numpy().tolist()\n",
    "cities['type_probabilities'] = cities[['mean_prob_cluster_2', 'mean_prob_cluster_0', 'mean_prob_cluster_1', 'mean_prob_cluster_3']].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "consensus_label_majority\n",
       "0    0.442\n",
       "1    0.520\n",
       "2    0.533\n",
       "3    0.286\n",
       "Name: probability, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.rename(columns={\n",
    "    'GHS_urban_area_id': 'id',\n",
    "    'city_name': 'name',\n",
    "    'Region': 'region',\n",
    "    'GHS_population': 'population',\n",
    "    'GHS_population_growth': 'population_growth',\n",
    "    'GHS_population_density': 'population_density',\n",
    "    'GHS_population_density_growth': 'population_density_growth',\n",
    "    'GHS_GDP_PPP': 'gdp_ppp',\n",
    "    'GHS_GDP_PPP_growth': 'gdp_ppp_growth',\n",
    "    'GHS_critical_infra': 'critical_infrastructure',\n",
    "    'GHS_greenness_index': 'greenness_index',\n",
    "    'GHS_precipitation': 'precipitation',\n",
    "    'GHS_HDI': 'hdi',\n",
    "    'GHS_female_gender_index': 'female_gender_index',\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metric ranks and store as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cols = [\n",
    "    'id',\n",
    "    'name',\n",
    "    'country',\n",
    "    'region',\n",
    "    'type',\n",
    "    'n_studies',\n",
    "    'n_solutions',\n",
    "    'solution_domain_counts',\n",
    "    'probability',\n",
    "    'embedding',\n",
    "    'neighbors',\n",
    "    'neighbor_distances',\n",
    "    'type_probabilities',\n",
    "    'lat',\n",
    "    'lon',\n",
    "]\n",
    "\n",
    "metric_cols = [\n",
    "    'population',\n",
    "    'population_growth',\n",
    "    'population_density',\n",
    "    'population_density_growth',\n",
    "    'gdp_ppp',\n",
    "    'gdp_ppp_growth',\n",
    "    'critical_infrastructure',\n",
    "    'greenness_index',\n",
    "    'precipitation',\n",
    "    'hdd',\n",
    "    'cdd',\n",
    "    'hdi',\n",
    "    'female_gender_index',\n",
    "    'emissions',\n",
    "]\n",
    "\n",
    "for col in metric_cols:\n",
    "    cities[f\"{col}_pct\"] = (cities[col].rank(pct=True) * 100).round()\n",
    "\n",
    "pct_cols = [f\"{col}_pct\" for col in metric_cols]\n",
    "\n",
    "cities[info_cols + metric_cols + pct_cols].to_json('public/cities.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type    region       \n",
       "Mixed   Asia             0.43\n",
       "        Africa           0.30\n",
       "        South America    0.16\n",
       "        North America    0.04\n",
       "        Europe           0.03\n",
       "        Small Islands    0.03\n",
       "        Australasia      0.00\n",
       "Type 1  Asia             0.63\n",
       "        Africa           0.31\n",
       "        South America    0.04\n",
       "        Small Islands    0.02\n",
       "        North America    0.00\n",
       "        Europe           0.00\n",
       "Type 2  Asia             0.69\n",
       "        South America    0.12\n",
       "        Africa           0.08\n",
       "        Europe           0.06\n",
       "        North America    0.04\n",
       "        Small Islands    0.01\n",
       "Type 3  Europe           0.51\n",
       "        Asia             0.25\n",
       "        North America    0.19\n",
       "        South America    0.02\n",
       "        Australasia      0.02\n",
       "        Small Islands    0.01\n",
       "Type 4  Asia             0.60\n",
       "        Africa           0.28\n",
       "        South America    0.05\n",
       "        Europe           0.04\n",
       "        North America    0.03\n",
       "Name: region, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities.groupby('type')['region'].value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometries.to_csv('cities-lat-lon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
