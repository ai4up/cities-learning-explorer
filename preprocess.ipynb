{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/playground/lib/python3.10/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.4-CAPI-1.17.4) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/var/folders/jk/rndbblbs2fs0_symsqry30400000gn/T/ipykernel_32441/1290313361.py:7: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.resetwarnings()\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(funcName)s - %(message)s', datefmt='%I:%M:%S')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Climate solution counts per domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_solutions = pd.read_parquet('../cities-learning-dec/data/climate_solutions_typology/oa_sentence_solutions_relevant.parquet')\n",
    "abstract_city_mapping = pd.read_csv('../cities-learning-dec/data/geoparser/clean_places_augmented.csv', index_col=0)\n",
    "abstract_city_mapping['city_id'] = abstract_city_mapping['city_word_match_id'].fillna(abstract_city_mapping['city_intersection_id'])\n",
    "\n",
    "abstract_solutions = pd.merge(abstract_solutions, abstract_city_mapping[['id', 'city_id']], on='id')\n",
    "\n",
    "domain_to_solution_ids = {\n",
    "    \"Mobility\": [1, 2, 3, 4, 5, 6, 7],\n",
    "    \"Buildings\": [8, 9, 10, 11, 12, 13, 14, 15, 16],\n",
    "    \"Energy\": [17, 18, 19, 20, 21, 22, 23],\n",
    "    \"Thermal comfort and Heat stress management\": [24, 25, 26, 27],\n",
    "    \"Food provisioning systems\": [28, 29, 30, 31, 32, 33, 34],\n",
    "    \"Water\": [35, 36, 37, 38, 39, 40],\n",
    "    \"Waste management\": [41, 42, 43, 44],\n",
    "    \"Disaster and risk management\": [45, 46, 47, 48, 49, 50],\n",
    "    \"Carbon dioxide removal\": [51, 52, 53, 54],\n",
    "}\n",
    "\n",
    "for domain, solution_ids in domain_to_solution_ids.items():\n",
    "    abstract_solutions[domain] = abstract_solutions[[f'solution_{i}_match' for i in solution_ids]].max(axis=1)\n",
    "\n",
    "domains = list(domain_to_solution_ids.keys())\n",
    "abstract_solution_counts = abstract_solutions.groupby(['city_id','id'])[domains].max()\n",
    "city_solution_counts = abstract_solution_counts.groupby('city_id')[domains].sum()\n",
    "\n",
    "city_solution_counts['n_solutions'] = city_solution_counts.sum(axis=1)\n",
    "city_solution_counts['solution_domain_counts'] = city_solution_counts.to_dict(orient=\"records\")\n",
    "city_solution_counts = city_solution_counts[['n_solutions', 'solution_domain_counts']].rename_axis('GHS_urban_area_id').reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Nearest neighbors in embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_all_runs = []\n",
    "for i in range(30):\n",
    "    embeddings = joblib.load(f'../cities-learning-dec/clustering_models/latent_representation/latent_run_{i}.pkl')\n",
    "\n",
    "    emb_cols = [\"latent_0\", \"latent_1\", \"latent_2\", \"latent_3\"]\n",
    "    X = embeddings[emb_cols].values\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=51, metric=\"euclidean\")\n",
    "    nn.fit(X)\n",
    "\n",
    "    distances, indices = nn.kneighbors(X)\n",
    "\n",
    "    # Remove self-index (first column)\n",
    "    neighbor_indices = indices[:, 1:]\n",
    "    neighbor_distances = distances[:, 1:]\n",
    "\n",
    "    city_ids = embeddings[\"GHS_urban_area_id\"].astype(int).values\n",
    "\n",
    "\n",
    "    N, K = neighbor_indices.shape\n",
    "\n",
    "    # Repeat each city ID K times â†’ shape (N*K,)\n",
    "    city_a = np.repeat(city_ids, K)\n",
    "\n",
    "    # Flatten neighbors and distances (already in the correct order)\n",
    "    city_b = city_ids[neighbor_indices].ravel()\n",
    "    dist = neighbor_distances.ravel()\n",
    "\n",
    "    nn_long = pd.DataFrame({\n",
    "        \"city_a\": city_a,\n",
    "        \"city_b\": city_b,\n",
    "        \"distance\": dist\n",
    "    })\n",
    "    nn_all_runs.append(nn_long)\n",
    "\n",
    "nn_long = pd.concat(nn_all_runs)\n",
    "\n",
    "nn = (\n",
    "    nn_long.groupby(['city_a', 'city_b'], as_index=False)\n",
    "          .distance.mean()\n",
    ")\n",
    "\n",
    "nn_top20 = (\n",
    "    nn.sort_values(['city_a', 'distance'])\n",
    "     .groupby('city_a')\n",
    "     .head(20)\n",
    "     .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "nn_top20['distance'] = nn_top20['distance'].round(4)\n",
    "nn_top20['distance'] = nn_top20['distance'].round(4)\n",
    "\n",
    "nn_wide = (\n",
    "    nn_top20\n",
    "    .groupby(\"city_a\")\n",
    "    .agg({\n",
    "        \"city_b\": list,\n",
    "        \"distance\": list\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "nn_wide = nn_wide.rename(columns={\n",
    "    \"city_a\": \"GHS_urban_area_id\",\n",
    "    \"city_b\": \"neighbors\",\n",
    "    \"distance\": \"neighbor_distances\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging all data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_characteristics = pd.read_parquet('../cities-learning-dec/data/clustering_data_clean/GHS_UCDB_2024_preproc_2025_04_09_uci_and_nan_imputation_add_vars_included.parquet')\n",
    "city_assignment_proba = pd.read_csv('../cities-learning-dec/data/clustering_results/dec_clusters_k4.csv')\n",
    "city_types = pd.read_csv('../cities-learning-dec/data/clustering_results/cities_by_regional_type_clean.csv', index_col=0).rename(columns={'city_id': 'GHS_urban_area_id'})\n",
    "embeddings = joblib.load('../cities-learning-dec/clustering_models/latent_representation/latent_run_0.pkl')\n",
    "city_socioeconomics = pd.read_csv('../cities-learning-dec/data/GHS_UCDB_GLOBE_R2024A_V1_0/socioeconomic.csv')[['ID_UC_G0', 'SC_SEC_GDF_2020', 'SC_SEC_HDI_2020']].rename(columns={'ID_UC_G0': 'GHS_urban_area_id', 'SC_SEC_GDF_2020': 'GHS_female_gender_index', 'SC_SEC_HDI_2020': 'GHS_HDI'})\n",
    "\n",
    "emissions = pd.read_csv('../cities-learning-dec/data/emissions/balance_sheet.csv')\n",
    "emissions = emissions[emissions['Year'] == 2022][['ID_UC_G0', 'ODIAC']].rename(columns={'ID_UC_G0': 'GHS_urban_area_id', 'ODIAC': 'total_emissions'})\n",
    "\n",
    "geometries = gpd.read_parquet('../cities-learning-dec/data/clustering_data_clean/GHS_UCDB_2024_preproc_2025_04_03.parquet', columns=['GHS_urban_area_id', 'geometry'])\n",
    "geometries['lat'] = geometries.centroid.to_crs(epsg=4326).y\n",
    "geometries['lon'] = geometries.centroid.to_crs(epsg=4326).x\n",
    "geometries = geometries[['GHS_urban_area_id', 'lat', 'lon']]\n",
    "\n",
    "cities = pd.merge(city_characteristics, city_types, on='GHS_urban_area_id')\n",
    "cities = pd.merge(city_assignment_proba, cities, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, embeddings, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, nn_wide, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, city_socioeconomics, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, geometries, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, city_solution_counts, on='GHS_urban_area_id', how='left')\n",
    "cities = pd.merge(cities, emissions, on='GHS_urban_area_id', how='left')\n",
    "\n",
    "cities['emissions'] = cities['total_emissions'] / cities['GHS_population']\n",
    "cities['city_name'].fillna('Unknown', inplace=True)\n",
    "cities['embedding'] = cities[['latent_0', 'latent_1', 'latent_2', 'latent_3']].to_numpy().tolist()\n",
    "cities['type_probabilities'] = cities[['mean_prob_cluster_2', 'mean_prob_cluster_3', 'mean_prob_cluster_0', 'mean_prob_cluster_1']].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = cities.rename(columns={\n",
    "    'GHS_urban_area_id': 'id',\n",
    "    'city_name': 'name',\n",
    "    'Region': 'region',\n",
    "    'cluster_name': 'type',\n",
    "    'GHS_population': 'population',\n",
    "    'GHS_population_growth': 'population_growth',\n",
    "    'GHS_population_density': 'population_density',\n",
    "    'GHS_population_density_growth': 'population_density_growth',\n",
    "    'GHS_GDP_PPP': 'gdp_ppp',\n",
    "    'GHS_GDP_PPP_growth': 'gdp_ppp_growth',\n",
    "    'GHS_critical_infra': 'critical_infrastructure',\n",
    "    'GHS_greenness_index': 'greenness_index',\n",
    "    'GHS_precipitation': 'precipitation',\n",
    "    'GHS_HDI': 'hdi',\n",
    "    'GHS_female_gender_index': 'female_gender_index',\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metric ranks and store as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_cols = [\n",
    "    'id',\n",
    "    'name',\n",
    "    'country',\n",
    "    'region',\n",
    "    'type',\n",
    "    'n_studies',\n",
    "    'solution_domain_counts',\n",
    "    'probability',\n",
    "    'embedding',\n",
    "    'neighbors',\n",
    "    'neighbor_distances',\n",
    "    'type_probabilities',\n",
    "    'lat',\n",
    "    'lon',\n",
    "]\n",
    "\n",
    "metric_cols = [\n",
    "    'population',\n",
    "    'population_growth',\n",
    "    'population_density',\n",
    "    'population_density_growth',\n",
    "    'gdp_ppp',\n",
    "    'gdp_ppp_growth',\n",
    "    'critical_infrastructure',\n",
    "    'greenness_index',\n",
    "    'precipitation',\n",
    "    'hdd',\n",
    "    'cdd',\n",
    "    'hdi',\n",
    "    'female_gender_index',\n",
    "    'emissions',\n",
    "]\n",
    "\n",
    "for col in metric_cols:\n",
    "    cities[f\"{col}_pct\"] = (cities[col].rank(pct=True) * 100).round()\n",
    "\n",
    "pct_cols = [f\"{col}_pct\" for col in metric_cols]\n",
    "\n",
    "cities[info_cols + metric_cols + pct_cols].to_json('cities-learning-explorer/public/cities.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
